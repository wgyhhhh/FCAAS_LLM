# 1 导言

大语言模型已经成为解决开放式问题的最强大方法之一，并标志着机器学习中的范式转变。然而，确保他们的安全性和对齐性仍然是一项公认的重大挑战，涉及多方利益相关者，包括私营人工智能实验室([Leike等](https://openai.com/blog/our-approach-to-alignment-research); [Anthropic等, 2023](https://www.anthropic.com/news/core-views-on-ai-safety); [Frontier Model Forum, 2023](https://www.frontiermodelforum.org/updates/announcing-the-frontier-model-forum/))、国家和国际政府组织([White House, 2023](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/); [Office, 2023](https://www.gov.uk/government/news/prime-minister-calls-for-global-responsibility-to-take-ai-risks-seriously-and-seize-its-opportunities); [Board, 2023](https://www.un.org/sites/un2.un.org/files/ai_advisory_body_interim_report.pdf))以及研究学术界([Bengio等, 2023](https://arxiv.org/pdf/2310.17688v2); [FAccT, 2023](https://facctconference.org/2023/harm-policy); [CAIS, 2023](https://www.safe.ai/statement-on-ai-risk); [CHAI, Far.ai and Ditchley Foundation, 2023](https://humancompatible.ai/news/2023/10/31/prominent-ai-scientists-from-china-and-the-west-propose-joint-strategy-to-mitigate-risks-from-ai/))。事实上，确保任何基于深度学习的系统具有安全性和对齐性都极为困难([Ngo等, 2023](https://arxiv.org/pdf/2209.00626))。但这一挑战对LLM而言尤为严峻，因其规模庞大([Sanh等, 2019](https://arxiv.org/pdf/1910.01108))，且能力日益广泛([Bubeck等, 2023](https://arxiv.org/pdf/2303.12712); [Morris等, 2023](https://arxiv.org/pdf/2311.02462))。此外LLM能力的快速进步不仅拓展了其潜在应用的场景，也放大了其可能造成的社会危害([Weidinger等, 2021](https://arxiv.org/pdf/2112.04359); [Ganguli等, 2022](https://arxiv.org/pdf/2202.07785); [Brihane等, 2023](https://arxiv.org/pdf/2306.13141); [Chan等, 2023a](https://arxiv.org/pdf/2302.10329))。

