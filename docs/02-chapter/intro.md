# 2 对LLM的科学理解

当前许多安全性和对齐性挑战源于我们队大语言模型(LLMs)科学认知的不足。若能深化对LLM的理解，将有助于更准确地评估当前及未来LLMs的风险，并设计有效的干预措施来降低这些风险。科学理解同样是系统可信度的重要因素-尤其是对复杂系统而言。桥梁设计、飞行器设计、核反应堆设计、航天器设计等经典案例表明，这些复杂系统的安全性与可信度高度依赖于科学理解。如果缺乏对基本原理的科学理解，要确保这些系统的可靠性即使并非完全不可能，也将变得极其困难。

LLMs展现出了复杂系统的诸多典型特征([Holtzman等, 2023](https://arxiv.org/pdf/2308.00189); [Steinhardt等, 2023](https://bounded-regret.ghost.io/complex-systems-are-hard-to-control/); [Hendrycks等, 2023](https://arxiv.org/pdf/2411.01042))，其中最显著的是涌现行为(emergent behaviors)([Wei等, 2022](https://arxiv.org/pdf/2206.07682); [Park等, 2023](https://arxiv.org/pdf/2304.03442)). 这种类复杂系统的特性意味着，仅依赖"评估"可能无法充分保障安全性与可靠性(参见2.2.3节与3.3节)，亟需突破对大语言模型表层行为的观测，从根本上理解这些行为的产生机制([Holtzman等, 2023](https://arxiv.org/pdf/2308.00189))。

理解大语言模型是一项重大的科学挑战，然而基于本文的整体研究主题，我们重点关注与安全性有关的方面(见表1)。解决这些挑战将有助于为更安全的LLM开发或部署实践提供依据；但要将此处的洞见转换为实际建议，仍需开展更多工作。

科学理解可以呈现多种不同的形式([Adams](https://dl.acm.org/doi/10.1002/sys.21255))。事实上，我们所识别的挑战本身就允许多样化的研究风格。部分挑战本质上就是高度理论性的(例如，**Transformers的计算极限是多少(第2.4.5节)**或者**理解规模化法则(scaling laws)(第2.3.1节)**)，而另一些可能需要更实证的方法来解决(例如，**单智能体训练对多智能体交互的影响尚不清楚(第2.6.1节)**)。总体而言，我们提出的挑战大多聚焦于建立对大语言模型的定性理解。定性理解往往能形成定量分析所无法实现的普遍规律，因此对LLMs的快速迭代可能更具鲁棒性。这种定性认知的优越性早在50年前就被Herbert Simon和Allen Newell在1975年图灵奖演讲中盛赞了-巧合的是，该演讲同样是关于人工智能系统的设计与理解议题([Newell和Simon](https://dl.acm.org/doi/10.1145/360018.360022))。

> 表1：第2节（大语言模型的科学理解）所讨论挑战的概述。需要强调的是，本概述是该节内容的**高度浓缩摘要**，因此不能替代对相应章节的完整阅读。

| 挑战      | 介绍  |
|:-------------:|:-----:|
|   上下文学习是黑盒子   | 我们尚未充分理解以下关键问题：大规模训练如何以及为何会催生上下文学习能力；大型语言模型（LLMs）中上下文学习的底层机制是什么；LLMs的上下文学习在多大程度上源于mesa优化（mesa-optimization）；以及该能力与现有学习算法的关联性。|
| 能力难以评估与理解 | 由于多种原因，准确评估和理解大型语言模型（LLMs）的能力存在困难。首先，LLM的能力呈现与人类能力不同的"形态"；这意味着用于理解和评估人类能力的认知框架可能并不适用于理解LLM的能力。此外，"能力"这一概念缺乏严格的定义，导致难以对LLM能力进行形式化断言及其评估。若想更好地理解LLM能力，我们还需克服现有评估方法中的根本缺陷，例如基准测试无法区分对齐失败与能力不足的问题。同时，我们需要改进评估工具以衡量LLM的泛化能力，并开发新方法来更有效地评估模型在结构化支持下的表现。 |
| 规模对能力的影响尚未得到充分表征 | 理解和预测规模对大型语言模型（LLM）能力的影响面临多重挑战，主要包括：1.对经验性缩放规律的理论认识尚不完善；2.对缩放极限及学习表征如何受规模影响的认知有限；3.因缺乏形式化定义，关于"涌现"能力的讨论存在混淆；4.针对任务特定缩放规律的发现方法研究仍处于早期阶段。 |
| 对推理能力的定性理解仍显不足 | 目前，我们对于大语言模型（LLM）中推理能力的涌现机制及其受模型规模影响的认知尚不充分，因此难以对未来LLM的推理能力做出可靠预测。亟需开展研究以揭示其推理的内在机制，深化对LLM非演绎推理能力的理解，并进一步探究Transformer架构的计算极限。 |
| 具有自主意识的大语言模型带来新型风险 | 由于能力提升（通过增强各类可操作性的访问等途径）和自主性增加，大语言模型智能体（LLM-agent）可能带来全新的对齐与安全风险。基于自然语言的指令存在欠规范性问题，导致LLM智能体执行的动作可能产生负面副作用。目标导向性可能使LLM智能体表现出奖励破解、欺骗和权力攫取等不良行为，并使得对LLM智能体的稳健监督与监测变得尤为困难。 |
| 多智能体安全性无法由单智能体安全性保证 | 在多智能体环境中确保有利结果可能面临多重挑战。首先，目前尚不完全理解单智能体训练如何影响多智能体环境中LLM智能体的行为。其次，LLM智能体的基础性可能导致关联性故障。此外，LLM智能体间的共谋可能产生不良外部性。最后，现有多智能体强化学习研究能在多大程度上改善LLM智能体在多智能体环境中的对齐性——特别是解决社会困境方面——仍不明确。|
| 安全性与性能的权衡关系尚未得到充分理解 | 在任何工程系统的设计中，安全性与性能的权衡通常是不可避免的；然而，对于基于大语言模型（LLM）的系统，这种权衡尚未得到充分理解。当前亟需开展以下工作：设计更优的安全度量指标、刻画不同场景下的安全-性能权衡关系，并深入探究哪些安全-性能权衡具有本质性（因而在实践中无法规避）。最后，研究能实现安全性与性能帕累托改进的方法可能具有重要价值。 |
