## 2.1 上下文学习是黑盒子

上下文学习是一种无需对模型参数进行任何更新却可以让LLM根据prompt中提供的信息(例如示例、推理轨迹)来学习新任务或者改进现有任务的能力([Brown等,2020](https://arxiv.org/pdf/2005.14165),[Kaplan等,2020](https://arxiv.org/pdf/2001.08361))，上下文学习是一种高度灵活和高效的学习范式-它已被利用指导LLM以期望的方式表现能力([Lin等,2023](https://arxiv.org/pdf/2312.01552))，越狱LLMs([Wei等,2023](https://arxiv.org/pdf/2310.06387);[Xhonneux等,2024](https://arxiv.org/pdf/2402.05723))以及创建高性能的LLM智能体([Wang等,2023](https://arxiv.org/pdf/2305.16291))。**然而，尽管上下文学习这种动态性质有助于设计成熟的LLM系统，但是上下文学习的黑盒性质也使得在确保LLM安全性和对齐性方面尤为困难**([Wolf等，2023](https://arxiv.org/pdf/2304.11082),[Milliere, 2023](https://arxiv.org/pdf/2311.02147))。如果我们不了解上下文学习是如何工作的，就会使得预测LLM在部署中的行为变得困难；例如，它可能启用新的危险功能或绕过安全措施([Anil等,2024](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf))。因此，我们迫切需要了解上下文学习的机制、上下文学习的局限性以及与与上下文学习相关的安全影响。本节介绍了为解释上下文学习而提出的各种理论和方法的问题，并强调了需要解决的几个关键问题。

### 2.1.1 ICL是复杂的模式匹配吗？ 

有两种理论来解释transformer中上下文学习的工作机制。第一种理论认为，上下文学习是一组与训练分布紧密相关的预学习模型匹配启发式算法(pre-learned pattern-matching heuristics)。根据这种观点，一些工作提出了上下文学习作为隐式模型推理的解释([Xie等,2022]();[Wang等,2023b]();[Wies等,2023]())；作为任务推理([Min等,2022]();[Todd等,2023]();[Hendel等,2023]();[Bigelow等,2023]())；或如模板电路(template circuits)的学习(在训练期间)，这些模板电路会被自适应地检索并与提示中的标记重新绑定([Saminathan等,2023]())。然而，这些理论目前仅能部分解释上下文学习。上述所有研究仅解释了基于说明样例(demonstrations)的上下文学习，而上下文学习也可能来自其他类型的反馈，例如交互式反馈([Mehrabi等,2023]()；[Wang等,2023a]())。这些研究也没有解释上下文中的多任务学习和任务的顺序学习([Zhou等,2022](),第4和5节)，或上下文学习如何支持新任务的学习，例如在分布外(OOD)样本上的推理能力([Saparov等,2023]())。此外，一些研究，如[Zhang等,2023a]()和[Swaminathan等,2023]()，假设了特定的(更简单的)数据生成过程，而这些过程与自然语言的真实数据生成过程不同，我们需要进一步完善和改进现有理论，或开发新的理论，以便我们能够充分解释上下文学习的全部行为，包括上述提到的行为。

### 2.1.2 上下文学习是mesa优化(mesa-optimization)吗?

或者，ICL 可以被视为一种“学习型优化(learned optimization)”或“mesa优化(mesa-optimization)”([Hubinger等,2019]();[von Oswald等,2023]())，即在训练过程中，基础优化器学习使用transformer权重来表示另一个(学习)优化算法——以及一个(学习)目标函数——有效地允许transformer自动生成学习信号(基于上下文中的数据),并充当黑盒上下文学习器发挥作用([Kirsch 等人，2022]())。模型中mesa优化的出现取决于模型是否能够在其权重内实现一种学习算法。多项研究表明，transformer可以近似各种统计学习问题中的基于梯度的学习算法([Akyürek等,2022a]()；[von Oswald等,2022]()；[Garg等,2022]()；[Zhang等,2023b]()；[Ahn等,2023]())。[Bai等,(2023a)]()证明了一个具有$2L$层的transformer可以模拟两层前馈神经网络中$L$步的梯度下降。[Panigrahi等,(2023)z]()提出了一种transformer架构的扩展，该扩展可以在内部模拟较小transformer的微调过程。[Bai等(2023a)]()进一步表明，transformer可以实现上下文中的算法选择，即在推理时间，单个transformer可以根据提示中的信息，自适应地选择不同的学习算法来解决给定的任务(例如，对于回归任务采用逻辑回归，对于分类任务采用逻辑分类)。

这些研究表明，transformers原则上能够在受控实验中实现mesa优化。然而，目前尚不清楚当transformer使用更复杂的目标(例如语言建模目标)训练的transformers是否以及何时会学习进行mesa优化。具体而言，一个关键的未知问题是：transformers支持哪些mesa目标来实现mesa优化?现有工作提供了大量证据，表明transformers可以通过mesa优化解决简单的监督学习任务。然而，迄今为止，对transformers是否可以通过mesa优化解决更复杂的上下文学习任务的研究非常有限([Lin等,2023b]())；未来的研究应该探索更贴近真实世界语言建模结构的任务。此外，即使对于一些已经有深入研究的学习任务(例如线性回归)，当前文献对于transformers执行的是哪种学习算法仍然存在分歧([von Oswald,2022]()；[Fu等,2023a]())。为了解决这一争议，需要进一步研究以区分影响transformers所实现学习算法的各种因素([Zhong等,2023a]())。未来的工作还可以开发更严格和通用的方法来区分mesa优化和其他形式的上下文学习，并研究上下文示例分布在决定是否发生mesa优化以及理解由此产生的mesa优化器的归纳偏置方面的实际重要性。 

### 2.1.3 哪些行为可以在上下文中指定？

目前尚不清楚哪些功能可以通过上下文学习中实现。更具体地说，给定一个预训练模型，我们可以提示它执行哪些任务？如果总是能找到一个prompt来引导LLM执行任何任务，那么这可能表明越狱(Jailbreak，参见3.5节)是一个无法解决的问题([Millière，2023]())。从根本上说，这是一个关于上下文内的泛化问题，即一个固定的模型是否可以通过适当的提示转换为任意精度的近似器，以逼近任意(连续)函数。虽然众所周知，当经过训练后，transformers是通用逼近器([Yun et al., 2019]())，最近还证明了状态空间模型也是如此([Li等,2022a]()；[Wang and Xue, 2023]()；[Cirone等,2024]())，但它们在上下文中的能力尚不明确。[Wang等,(2023c)]()证明了使用transformers可以在上下文中进行通用近似，但他们的构建要求所有可能的函数都编码在模型权重中，这导致模型变得不切实际地庞大。然而，[Petrov等,(2024)]()证明不需要记忆，一个相对较小的模型就可以在上下文中成为通用近似器。这个结果表明，对于一个现实规模的模型，可能无法对其进行安全保护(即以一种无法越狱的方式进行安全微调)。 

上下文中的通用逼近理论仍处于初级阶段，上述结果的实际安全性和安全性影响也尚不明确。例如，上述模型需要为预训练模型设计特定手工权重。目前尚不清楚通过在典型数据集上使用梯度下降进行学习，是否可以获得通用逼近能力，因此，这种行为在现实世界模型中是否可能发生也并不确定。此外，[Petrov等]()在构建模型所需的prompt长度是不切实际的长。然而，通过利用模型中已有的知识和技能，可以缩短这一长度[Petrov等,2023a]()。因此，研究预训练数据对上下文通用逼近的影响，可以帮助理解LLM上下文学习能力在现实世界中的安全性和其影响。上述所有结果都依赖于Transformer架构中的注意力机制，因此无法直接递归到其他循环模型。因此，理解其他架构的上下文逼近特性是另一个开放性的问题，目前几乎没有相关工作研究[Lee等，2023a]()。

### 2.1.4 基于场景的上下文学习机理理解 

当前的可解释性技术繁华性不足，无法通过基于可解释性的方法来全面理解LLM中上下文学习的机制(参见第3.4节)。然而，它们仍然可以利用开发基于场景的上下文学习机制理解([Olsson等，2022]()；[Reddy，2023]())，即在对prompt结构和通过上下文学习执行的任务施加人为限制时，在LLM中识别对ICL至关重要的。这样的案例研究可以深入了解模型中不同计算结构所发挥的相对作用，以及上下文学习机制在不同任务中的变化方式。例如，[Todd等(2023)]()研究了上下文学习基于抽取和抽象的自然语言处理中的上下文学习，发现少量的注意力头(称为“功能向量”)负责传输任务的紧凑表示，然后触发任务的执行。类似，[Merullo等,(2023a)]()表明，在通常研究的上下文学习任务中，推断实体之间的关系（例如推断一个国家的首都），中到晚期的前馈层在识别和揭示上下文中包含的相关信息方面起着关键的作用，这些信息对于推断关系和完成任务是必需的。[Halawi等,(2023)]()研究了在分类任务上的上下文学习，其中演示数据具有错误标签，从而与LLM的先验知识相冲突。这使得[Halawi等]()发现了虚假归纳头(false induction heads)；即LLM晚期层中的注意力头，它们会关注并复制演示中的错误信息。 

这些初步证据表明，针对特定任务类的案例研究可能是一种有效的方法，其有助于建立对LLM中上下文学习机制的理解，并可能有助于确定上下文学习在不同任务中的变化方式和原因，以及特定架构的归纳偏差如何影响上下文学习。未来的研究可以分析更大的LLM、各种任务类型和各种提示风格。还可以利用可解释性技术来解释文献中发现的上下文学习的特异性现象，例如为什么ICL有时在给定测试分布的示例时表现更差([Saparov等人，2023]())或为什么不同规模的LLM在处理上下文中的虚假信息时有所不同([Wei等人，2023b]())。 

### 2.1.5理解预训练数据分布对上下文学习的影响

上下文学习能力很大程度受到预训练数据分布结构的影响。任务分布的特殊属性，如任务多样性([Raventós 等,2023]()；[Kirsch等，2022]())，“突发性”([Chan等，2022]())或组合结构([Hahn和Goyal，2023]()),可能是上下文学习出现的关键因素。然而，这些发现主要局限于相对简单的设置，需要进一步的工作来验证它们在真实世界语言建模环境中的正确性。大规模文本数据集是否真正满足这些标准？如果这些标准确实被文本数据集满足，哪些标准实际上负责LLM中的上下文学习？对于其中一些标准(例如“突发性”)，这些问题可以通过对流行的数据集(*The Pile* ([Gao 等人，2020]())，*RedPajama* ([Together Computer，2023]()))等来回答这些问题。然而对于其他标准(如任务多样性)，基于分析的方法可能并不适用，因为在非结构化文本数据集中可能没有方法来追踪和衡量这些标准。在这种情况下，另一种策略可能是以可控的方式创建各种合成文本数据集，并检测在这些数据集上训练的语言模型中上下文学习的出现，以更好地理解上下文学习出现的充分和必要条件。 

### 2.1.6 理解设计选择对上下文学习的影响

上下文学习的表现受到多种因素的影响，这些因素包括但不限于模型规模、预训练数据集规模、预训练计算量以及指令调优。从以往的文献中可以清楚地了解到，上下文学习对多种因素非常敏感，特别是，几项研究指出，上下文学习的表现对LLM规模大小十分敏感。例如，[Wei等,(2022a)]()；[Brown等,(2020)]()；[Kaplan等,(2020)]()指出，上下文学习是一种新兴能力；[Akyürek等,(2022a)]()发现，基于Transformer深度的不同，上下文学习会表现出模拟不同学习算法的现象；[Wei等,(2023b)]()发现，更大的 LLM 具有更强的语义先验（即零样本(zero-shot)性能更好），但也更容易受到上下文信息的干扰从而削弱语义先验。[Wei 等,(2023b)]()进一步表明指令调整不成比例地加强了语义先验；因此，指令调整会降低上下文信息覆盖语义先验的倾向。同时，[Singh等,2023]()认为上下文学习是一种短暂的现象，延长训练会导致上下文学习消失，转而支持“在权重内学习"([Chan等，2022]())。目前，我们对上下文学习为什么对各种设计和训练选择敏感，以及这些选择如何影响上下文学习背后的机制缺乏深入理解。特别是各种训练设计决策，例如指令调整或延长训练，如何影响上下文学习，可能会为我们提供调节LLM中上下文学习的强度的工具。这可能有助于缓解LLM由于其强大的上下文学习能力而带来的安全风险([Wolf等，2023]()；[Millière，2023]())。 

!!! note 
     上下文学习的动态性和灵活性是LLM成功的关键，因为它允许LLM熟练地改进已知任务，并学习执行新的任务。随着LLM规模的进一步扩大和在上下文学习能力的进一步提升，上下文学习很可能会发挥更加突出的作用。然而，从对齐和安全的角度来看，上下文学习的黑盒性质仍然存在风险，因此迫切需要更好地理解上下文学习背后的机制。目前已经有几种“理论”被提出，为LLM中的上下文学习如何工作提供了合理的解释。然而，LLM中上下文学习的底层机制仍然不甚了解。我们强调了一些研究问题，这些研究问题对于深入理解上下文学习的底层机制可能具有重要作用。  
        
     1. 能否将上下文学习的不同理论化解释(如复杂的模式匹配或mesa优化)扩展到解释LLM所表现出的ICL行为的全部范围？
   
     2. 上下文学习与现有学习范式之间有哪些关键差异和共同点？以往的研究大多从少样本监督学习的角度来探讨上下文学习。然而，在实践中，上下文学习有时会表现出与监督学习截然不同的特性，并且能够从除标注示例之外的数据中学习，例如交互式反馈、解释或推理模式。
   
     3. 哪些学习算法可以由Transformer在上下文中实现？虽然早期的研究(例如[Akyürek等,2022a]())认为Transformer实现了基于梯度下降的学习算法，但最近的研究([Fu等，2023a]())表明，Transformer也可以实现更高阶的迭代学习算法，例如迭代牛顿法。
   
     4. 有哪些最佳的抽象设置，能够更好地模拟语言建模在现实世界中的结构，同时又保持可处理性？当前的一些简单设置(例如学习求解线性回归)过于简单，可能导致研究结果无法推广到真实的大型语言模型。
   
     5. 不同架构在“上下文”中作为通用逼近器的适用程度如何？我们能否更好地刻画实践中获得的模型在上下文中可学习的函数？预训练数据和训练目标如何影响模型在上下文中的通用逼近能力？
   
     6. 基于可解释性的分析如何有助于对上下文学习机制的总体理解？这种方法能否用来解释与上下文学习相关的各种现象，例如为什么上下文学习在不同任务上的表现会有所不同，特定架构的归纳偏置如何影响上下文学习，不同提示风格如何影响上下文学习等？
   
     7. 哪些大规模文本数据集的特性导致了使用自回归目标训练的大型语言模型中涌现出上下文学习的能力？
   
     8. 预训练pipeline的不同组成部分（例如预训练数据集构建、模型大小、预训练浮点运算次数、学习目标）以及微调pipeline（例如，instruction-tuning、RLHF）对上下文学习有何影响？如何利用这种理解来开发调节LLM中上下文学习的技术？