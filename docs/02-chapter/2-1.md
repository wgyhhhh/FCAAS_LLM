## 2.1 上下文学习是黑盒子

上下文学习是一种无需对模型参数进行任何更新却可以让LLM根据prompt中提供的信息(例如示例、推理轨迹)来学习新任务或者改进现有任务的能力([Brown等,2020](https://arxiv.org/pdf/2005.14165),[Kaplan等,2020](https://arxiv.org/pdf/2001.08361))，上下文学习是一种高度灵活和高效的学习范式-它已被利用指导LLM以期望的方式表现能力([Lin等,2023](https://arxiv.org/pdf/2312.01552))，越狱LLMs([Wei等,2023](https://arxiv.org/pdf/2310.06387);[Xhonneux等,2024](https://arxiv.org/pdf/2402.05723))以及创建高性能的LLM智能体([Wang等,2023](https://arxiv.org/pdf/2305.16291))。**然而，尽管上下文学习这种动态性质有助于设计成熟的LLM系统，但是上下文学习的黑盒性质也使得在确保LLM安全性和对齐性方面尤为困难**([Wolf等，2023](https://arxiv.org/pdf/2304.11082),[Milliere, 2023](https://arxiv.org/pdf/2311.02147))。如果我们不了解上下文学习是如何工作的，就会使得预测LLM在部署中的行为变得困难；例如，它可能启用新的危险功能或绕过安全措施([Anil等,2024](https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf))。因此，我们迫切需要了解上下文学习的机制、上下文学习的局限性以及与与上下文学习相关的安全影响。本节介绍了为解释上下文学习而提出的各种理论和方法的问题，并强调了需要解决的几个关键问题。

### 2.1.1 ICL是复杂的模式匹配吗？ 

有两种理论来解释transformer中上下文学习的工作机制。第一种理论认为，上下文学习是一组与训练分布紧密相关的预学习模型匹配启发式算法(pre-learned pattern-matching heuristics)。根据这种观点，一些工作提出了上下文学习作为隐式模型推理的解释([Xie等,2022]();[Wang等,2023b]();[Wies等,2023]())；作为任务推理([Min等,2022]();[Todd等,2023]();[Hendel等,2023]();[Bigelow等,2023]())；或如模板电路(template circuits)的学习(在训练期间)，这些模板电路会被自适应地检索并与提示中的标记重新绑定([Saminathan等,2023]())。然而，这些理论目前仅能部分解释上下文学习。上述所有研究仅解释了基于说明样例(demonstrations)的上下文学习，而上下文学习也可能来自其他类型的反馈，例如交互式反馈([Mehrabi等,2023]()；[Wang等,2023a]())。这些研究也没有解释上下文中的多任务学习和任务的顺序学习([Zhou等,2022](),第4和5节)，或上下文学习如何支持新任务的学习，例如在分布外(OOD)样本上的推理能力([Saparov等,2023]())。此外，一些研究，如[Zhang等,2023a]()和[Swaminathan等,2023]()，假设了特定的(更简单的)数据生成过程，而这些过程与自然语言的真实数据生成过程不同，我们需要进一步完善和改进现有理论，或开发新的理论，以便我们能够充分解释上下文学习的全部行为，包括上述提到的行为。

### 2.1.2 上下文学习是mesa优化(mesa-optimization)吗?

或者，ICL 可以被视为一种“学习型优化(learned optimization)”或“mesa优化(mesa-optimization)”([Hubinger等,2019]();[von Oswald等,2023]())，即在训练过程中，基础优化器学习使用transformer权重来表示另一个(学习)优化算法——以及一个(学习)目标函数——有效地允许transformer自动生成学习信号(基于上下文中的数据),并充当黑盒上下文学习器发挥作用([Kirsch 等人，2022]())。模型中mesa优化的出现取决于模型是否能够在其权重内实现一种学习算法。多项研究表明，transformer可以近似各种统计学习问题中的基于梯度的学习算法([Akyürek等,2022a]()；[von Oswald等,2022]()；[Garg等,2022]()；[Zhang等,2023b]()；[Ahn等,2023]())。[Bai等,(2023a)]()证明了一个具有$2L$层的transformer可以模拟两层前馈神经网络中$L$步的梯度下降。[Panigrahi等,(2023)z]()提出了一种transformer架构的扩展，该扩展可以在内部模拟较小transformer的微调过程。[Bai等(2023a)]()进一步表明，transformer可以实现上下文中的算法选择，即在推理时间，单个transformer可以根据提示中的信息，自适应地选择不同的学习算法来解决给定的任务(例如，对于回归任务采用逻辑回归，对于分类任务采用逻辑分类)。

这些研究表明，transformers原则上能够在受控实验中实现mesa优化。然而，目前尚不清楚当transformer使用更复杂的目标(例如语言建模目标)训练的transformers是否以及何时会学习进行mesa优化。具体而言，一个关键的未知问题是：transformers支持哪些mesa目标来实现mesa优化?现有工作提供了大量证据，表明transformers可以通过mesa优化解决简单的监督学习任务。然而，迄今为止，对transformers是否可以通过mesa优化解决更复杂的上下文学习任务的研究非常有限([Lin等,2023b]())；未来的研究应该探索更贴近真实世界语言建模结构的任务。此外，即使对于一些已经有深入研究的学习任务(例如线性回归)，当前文献对于transformers执行的是哪种学习算法仍然存在分歧([von Oswald,2022]()；[Fu等,2023a]())。为了解决这一争议，需要进一步研究以区分影响transformers所实现学习算法的各种因素([Zhong等,2023a]())。未来的工作还可以开发更严格和通用的方法来区分mesa优化和其他形式的上下文学习，并研究上下文示例分布在决定是否发生mesa优化以及理解由此产生的mesa优化器的归纳偏置方面的实际重要性。 

### 2.1.3 哪些行为可以在上下文中指定？

目前尚不清楚哪些功能可以通过上下文学习中实现。更具体地说，给定一个预训练模型，我们可以提示它执行哪些任务？如果总是能找到一个prompt来引导LLM执行任何任务，那么这可能表明越狱(Jailbreak，参见3.5节)是一个无法解决的问题([Millière，2023]())。从根本上说，这是一个关于上下文内的泛化问题，即一个固定的模型是否可以通过适当的提示转换为任意精度的近似器，以逼近任意(连续)函数。虽然众所周知，当经过训练后，transformers是通用逼近器([Yun et al., 2019]())，最近还证明了状态空间模型也是如此([Li等,2022a]()；[Wang and Xue, 2023]()；[Cirone等,2024]())，但它们在上下文中的能力尚不明确。[Wang等,(2023c)]()证明了使用transformers可以在上下文中进行通用近似，但他们的构建要求所有可能的函数都编码在模型权重中，这导致模型变得不切实际地庞大。然而，[Petrov等,(2024)]()证明不需要记忆，一个相对较小的模型就可以在上下文中成为通用近似器。这个结果表明，对于一个现实规模的模型，可能无法对其进行安全保护(即以一种无法越狱的方式进行安全微调)。 

上下文中的通用逼近理论仍处于初级阶段，上述结果的实际安全性和安全性影响也尚不明确。例如，上述模型需要为预训练模型设计特定手工权重。目前尚不清楚通过在典型数据集上使用梯度下降进行学习，是否可以获得通用逼近能力，因此，这种行为在现实世界模型中是否可能发生也并不确定。此外，[Petrov等]()在构建模型所需的prompt长度是不切实际的长。然而，通过利用模型中已有的知识和技能，可以缩短这一长度[Petrov等,2023a]()。因此，研究预训练数据对上下文通用逼近的影响，可以帮助理解LLM上下文学习能力在现实世界中的安全性和其影响。上述所有结果都依赖于Transformer架构中的注意力机制，因此无法直接递归到其他循环模型。因此，理解其他架构的上下文逼近特性是另一个开放性的问题，目前几乎没有相关工作研究[Lee等，2023a]()。

### 2.1.4 基于场景的上下文学习机理理解 

当前的可解释性技术繁华性不足，无法通过基于可解释性的方法来全面理解LLM中上下文学习的机制(参见第3.4节)。然而，它们仍然可以利用开发基于场景的上下文学习机制理解([Olsson等，2022]()；[Reddy，2023]())，即在对prompt结构和通过上下文学习执行的任务施加人为限制时，在LLM中识别对ICL至关重要的。这样的案例研究可以深入了解模型中不同计算结构所发挥的相对作用，以及上下文学习机制在不同任务中的变化方式。例如，[Todd等(2023)]()研究了上下文学习基于抽取和抽象的自然语言处理中的上下文学习，发现少量的注意力头(称为“功能向量”)负责传输任务的紧凑表示，然后触发任务的执行。类似，[Merullo等,(2023a)]()表明，在通常研究的上下文学习任务中，推断实体之间的关系（例如推断一个国家的首都），中到晚期的前馈层在识别和揭示上下文中包含的相关信息方面起着关键的作用，这些信息对于推断关系和完成任务是必需的。[Halawi等,(2023)]()研究了在分类任务上的上下文学习，其中演示数据具有错误标签，从而与LLM的先验知识相冲突。这使得[Halawi等]()发现了虚假归纳头(false induction heads)；即LLM晚期层中的注意力头，它们会关注并复制演示中的错误信息。 

这些初步证据表明，针对特定任务类的案例研究可能是一种有效的方法，其有助于建立对LLM中上下文学习机制的理解，并可能有助于确定上下文学习在不同任务中的变化方式和原因，以及特定架构的归纳偏差如何影响上下文学习。未来的研究可以分析更大的LLM、各种任务类型和各种提示风格。还可以利用可解释性技术来解释文献中发现的上下文学习的特异性现象，例如为什么ICL有时在给定测试分布的示例时表现更差([Saparov等人，2023]())或为什么不同规模的LLM在处理上下文中的虚假信息时有所不同([Wei等人，2023b]())。 

!!! note 说明
    明天继续更新


### 2.1.5理解预训练数据分布对上下文学习的影响

上下文学习的结构受到预训练数据分布结构的强烈调节。任务分布的特殊属性，如任务多样性（Raventós 等人，2023；Kirsch 等人，2022），“突发性”（Chan 等人，2022）或组合结构（Hahn 和 Goyal，2023）可能是 ICL 出现的关键因素。然而，这些发现主要局限于相对简单的设置，需要进一步的工作来验证它们在现实世界语言建模设置中的正确性。大型文本数据集是否真正满足这些标准？如果这些标准确实被文本数据集满足，哪些标准实际上对 LLM 中的 ICL 负责？对于其中一些标准（例如“突发性”），这些问题可以通过对流行的数据集（如 The Pile (Gao 等人，2020)，RedPajama (Together Computer，2023) 等）进行分析来回答。对于其他标准，如任务多样性，基于分析的方法可能不适合，因为可能没有明显的方法来跟踪和测量这种标准在非结构化文本数据集中的情况。在这种情况下，一种替代策略可能是以受控的方式创建各种合成文本数据集，并监控在这些数据集上训练的语言模型中 ICL 的出现，以更好地理解 ICL 出现的必要和充分条件。 

### 2.1.6 理解设计选择对上下文学习的影响

上下文学习受到LLM开发中涉及的各种设计和训练选择的影响，例如模型大小、预训练数据集大小、预训练计算、指令调整？ICL 对各种因素的敏感性，包括但不限于上述因素，是众所周知的。特别是，几项研究指出，ICL 的性能对 LLM 的规模非常敏感；Wei 等人（2022a）；Brown 等人（2020）；Kaplan 等人（2020）指出，ICL 是一种新兴能力；Akyürek 等人（2022a）发现，基于变压器的深度，变压器模拟不同学习算法的相变；Wei 等人（2023b）发现，更大的 LLM 具有更强的语义先验（即零样本性能更好），但也更有可能允许上下文信息覆盖语义先验。Wei 等人（2023b）进一步表明指令调整不成比例地加强了语义先验；因此，指令调整降低了上下文信息覆盖语义先验的倾向。同时，Singh等人（2023）认为ICL是一种暂时现象，延长训练会导致ICL消散，有利于“在权重”学习（Chan等人，2022）。目前，我们对为什么ICL对各种设计和训练选择敏感，以及这些选择如何影响ICL背后的机制缺乏深入理解。特别是，提高我们对各种训练设计决策，例如指令调整或延长训练，如何影响ICL的理解，可能会为我们提供工具来调节LLM中ICL的强度。这可能有助于减轻LLM由于其强大的上下文学习能力而带来的安全风险（Wolf等人，2023；Millière，2023）。 

!!! note 
     ICL的动态和灵活性是LLM成功的关键，因为它允许LLM熟练地改进已知任务，并学习执行新的任务。随着LLM的进一步扩展和在ICL方面变得更加熟练，ICL可能会获得更加突出的作用。然而，从对齐和安全的角度来看，ICL的黑盒性质是一种风险，因此迫切需要更好地理解ICL的机制。已经提出了几种“理论”，它们为LLM中的ICL如何工作提供了合理的解释。然而，LLM中ICL的底层机制仍然不甚了解。我们强调了一些研究问题，这些研究问题可能有助于解决对ICL底层机制的理解。  
        
     1. 不同的ICL理论，如复杂的模式匹配或mesaoptimization，是否可以扩展到解释LLMs展示的全部ICL行为？←
   
     2. ICL与现有学习范式之间的关键差异和共同点是什么？先前的工作大多是从少量监督学习的角度来研究ICL的。然而，在实践中，ICL有时会表现出与监督学习截然不同的行为，并且可以从标记示例以外的数据中学习，例如交互反馈、解释或推理模式。
   
     3. 变压器可以在上下文中实现哪些学习算法？虽然早期的研究（例如Akyürek等人，2022a）认为变压器实现基于梯度下降的学习算法，但最近的工作（Fu等人，2023a）表明，变压器还可以实现更高阶的迭代学习算法，例如迭代牛顿方法。
   
     4. 什么是研究ICL的最佳抽象设置，这些设置更好地反映了语言建模的现实结构，同时仍然易于处理？当前的玩具设置，例如学习解决线性回归问题，过于简单，可能导致研究结果无法转移到实际的LLM上。
   
     5. 在多大程度上，不同的架构是“上下文中的”通用逼近器？我们能否更好地描述模型在实践中可以学习的函数？预训练数据和训练目标如何影响模型的上下文通用逼近能力？
   
     6. 基于可解释性的分析如何有助于对ICL机制的普遍理解？这种方法是否可以用来解释与ICL相关的各种现象，例如为什么ICL性能在不同任务之间变化，特定架构的归纳偏差如何影响ICL，不同的提示风格如何影响ICL等？
   
     7. 大规模文本数据集的哪些特性导致了在自回归目标下训练的LLM中出现ICL？
   
     8. 预训练管道的不同组件（例如预训练数据集构建、模型大小、预训练FLOPS、学习目标）和微调管道的不同组件如何影响ICL？